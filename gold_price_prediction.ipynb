{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa20dcc-a9c9-4309-b499-8ba5256bd710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict, Any, Optional\n",
    "import logging\n",
    "\n",
    "# Machine Learning imports\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640c0078-f7f3-41d7-950a-3eecb4a31513",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Optimized Gold Price Prediction using Machine Learning\n",
    "=====================================================\n",
    "\n",
    "This module provides a comprehensive implementation for predicting gold price\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"darkgrid\", {\"grid.color\": \".6\", \"grid.linestyle\": \":\"})\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "\n",
    "class GoldPricePredictor:\n",
    "    \"\"\"\n",
    "    A comprehensive class for gold price prediction using machine learning.\n",
    "    \n",
    "    This class handles data loading, preprocessing, feature engineering,\n",
    "    model training, and evaluation with multiple algorithms.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_path: str = \"gold_price_data.csv\"):\n",
    "        \"\"\"\n",
    "        Initialize the GoldPricePredictor.\n",
    "        \n",
    "        Args:\n",
    "            data_path (str): Path to the CSV file containing gold price data\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.dataset = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.X_train_scaled = None\n",
    "        self.X_test_scaled = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.imputer = SimpleImputer(strategy='mean')\n",
    "        self.models = {}\n",
    "        self.feature_names = None\n",
    "        \n",
    "    def load_data(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load and validate the dataset.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Loaded dataset\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Loading data from {self.data_path}\")\n",
    "            self.dataset = pd.read_csv(self.data_path, parse_dates=[\"Date\"])\n",
    "            \n",
    "            # Basic validation\n",
    "            if self.dataset.empty:\n",
    "                raise ValueError(\"Dataset is empty\")\n",
    "            \n",
    "            logger.info(f\"Dataset loaded successfully. Shape: {self.dataset.shape}\")\n",
    "            logger.info(f\"Columns: {list(self.dataset.columns)}\")\n",
    "            \n",
    "            return self.dataset\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"Data file not found: {self.data_path}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading data: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def explore_data(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Perform exploratory data analysis.\n",
    "        \n",
    "        Returns:\n",
    "            Dict[str, Any]: Summary statistics and insights\n",
    "        \"\"\"\n",
    "        logger.info(\"Performing exploratory data analysis...\")\n",
    "        \n",
    "        # Basic info\n",
    "        info = {\n",
    "            'shape': self.dataset.shape,\n",
    "            'columns': list(self.dataset.columns),\n",
    "            'dtypes': self.dataset.dtypes.to_dict(),\n",
    "            'missing_values': self.dataset.isna().sum().to_dict(),\n",
    "            'skewness': self.dataset.drop(\"Date\", axis=1).skew().to_dict()\n",
    "        }\n",
    "        \n",
    "        # Display basic info\n",
    "        logger.info(f\"Dataset shape: {info['shape']}\")\n",
    "        logger.info(f\"Missing values: {info['missing_values']}\")\n",
    "        logger.info(f\"Skewness: {info['skewness']}\")\n",
    "        \n",
    "        return info\n",
    "    \n",
    "    def create_correlation_heatmap(self, save_path: Optional[str] = None):\n",
    "        \"\"\"Create and display correlation heatmap.\"\"\"\n",
    "        correlation = self.dataset.corr()\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(correlation, cmap='coolwarm', center=0, annot=True, fmt='.2f')\n",
    "        plt.title('Correlation Matrix Heatmap')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def preprocess_data(self, drop_slv: bool = True) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Preprocess the data including feature engineering and outlier removal.\n",
    "        \n",
    "        Args:\n",
    "            drop_slv (bool): Whether to drop the SLV column\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: Preprocessed dataset\n",
    "        \"\"\"\n",
    "        logger.info(\"Starting data preprocessing...\")\n",
    "        \n",
    "        # Create a copy to avoid modifying original data\n",
    "        data = self.dataset.copy()\n",
    "        \n",
    "        # Drop SLV column if requested\n",
    "        if drop_slv and 'SLV' in data.columns:\n",
    "            data = data.drop(\"SLV\", axis=1)\n",
    "            logger.info(\"Dropped SLV column\")\n",
    "        \n",
    "        # Set Date as index\n",
    "        data.set_index(\"Date\", inplace=True)\n",
    "        \n",
    "        # Create price trend feature using rolling mean\n",
    "        data[\"price_trend\"] = data[\"EUR/USD\"].rolling(window=20).mean()\n",
    "        \n",
    "        # Reset index to include Date as column\n",
    "        data.reset_index(inplace=True)\n",
    "        \n",
    "        # Handle skewed data with square root transformation\n",
    "        if data['USO'].skew() > 1.0:\n",
    "            data[\"USO\"] = data[\"USO\"].apply(lambda x: np.sqrt(x))\n",
    "            logger.info(\"Applied square root transformation to USO column\")\n",
    "        \n",
    "        # Remove outliers using percentile capping\n",
    "        numeric_columns = ['SPX', 'GLD', 'USO', 'EUR/USD']\n",
    "        for col in numeric_columns:\n",
    "            if col in data.columns:\n",
    "                data[col] = self._remove_outliers(data[col])\n",
    "        \n",
    "        logger.info(\"Data preprocessing completed\")\n",
    "        return data\n",
    "    \n",
    "    def _remove_outliers(self, column: pd.Series) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Remove outliers using percentile capping.\n",
    "        \n",
    "        Args:\n",
    "            column (pd.Series): Input column\n",
    "            \n",
    "        Returns:\n",
    "            pd.Series: Column with outliers capped\n",
    "        \"\"\"\n",
    "        upper_limit = column.quantile(0.95)\n",
    "        lower_limit = column.quantile(0.05)\n",
    "        \n",
    "        column_capped = column.copy()\n",
    "        column_capped.loc[column_capped > upper_limit] = upper_limit\n",
    "        column_capped.loc[column_capped < lower_limit] = lower_limit\n",
    "        \n",
    "        return column_capped\n",
    "    \n",
    "    def prepare_features(self, target_column: str = 'EUR/USD') -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        Prepare features and target for modeling.\n",
    "        \n",
    "        Args:\n",
    "            target_column (str): Name of the target column\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.Series]: Features and target\n",
    "        \"\"\"\n",
    "        logger.info(\"Preparing features and target...\")\n",
    "        \n",
    "        # Select features and target\n",
    "        X = self.dataset.drop(['Date', target_column], axis=1)\n",
    "        y = self.dataset[target_column]\n",
    "        \n",
    "        self.feature_names = X.columns.tolist()\n",
    "        logger.info(f\"Features: {self.feature_names}\")\n",
    "        logger.info(f\"Target: {target_column}\")\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def split_and_scale_data(self, X: pd.DataFrame, y: pd.Series, \n",
    "                           test_size: float = 0.2, random_state: int = 42) -> None:\n",
    "        \"\"\"\n",
    "        Split data into train/test sets and scale features.\n",
    "        \n",
    "        Args:\n",
    "            X (pd.DataFrame): Feature matrix\n",
    "            y (pd.Series): Target variable\n",
    "            test_size (float): Proportion of data for testing\n",
    "            random_state (int): Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        logger.info(\"Splitting and scaling data...\")\n",
    "        \n",
    "        # Split data\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # Scale features\n",
    "        self.X_train_scaled = self.scaler.fit_transform(self.X_train)\n",
    "        self.X_test_scaled = self.scaler.transform(self.X_test)\n",
    "        \n",
    "        # Handle missing values\n",
    "        self.X_train_scaled = self.imputer.fit_transform(self.X_train_scaled)\n",
    "        self.X_test_scaled = self.imputer.transform(self.X_test_scaled)\n",
    "        \n",
    "        logger.info(f\"Training set shape: {self.X_train.shape}\")\n",
    "        logger.info(f\"Test set shape: {self.X_test.shape}\")\n",
    "    \n",
    "    def train_lasso_model(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Train Lasso regression with polynomial features and hyperparameter tuning.\n",
    "        \n",
    "        Returns:\n",
    "            Dict[str, Any]: Model results and metrics\n",
    "        \"\"\"\n",
    "        logger.info(\"Training Lasso model with polynomial features...\")\n",
    "        \n",
    "        # Create pipeline\n",
    "        poly = PolynomialFeatures(degree=2)\n",
    "        lasso = Lasso()\n",
    "        pipeline = make_pipeline(poly, lasso)\n",
    "        \n",
    "        # Define parameter grid\n",
    "        param_grid = {\n",
    "            'lasso__alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1, 5, 10, 20, 30, 40]\n",
    "        }\n",
    "        \n",
    "        # Grid search\n",
    "        lasso_grid_search = GridSearchCV(\n",
    "            pipeline, param_grid, scoring='r2', cv=3, n_jobs=-1\n",
    "        )\n",
    "        lasso_grid_search.fit(self.X_train_scaled, self.y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_train_pred = lasso_grid_search.predict(self.X_train_scaled)\n",
    "        y_test_pred = lasso_grid_search.predict(self.X_test_scaled)\n",
    "        \n",
    "        # Metrics\n",
    "        train_r2 = r2_score(self.y_train, y_train_pred)\n",
    "        test_r2 = r2_score(self.y_test, y_test_pred)\n",
    "        test_rmse = np.sqrt(mean_squared_error(self.y_test, y_test_pred))\n",
    "        test_mae = mean_absolute_error(self.y_test, y_test_pred)\n",
    "        \n",
    "        results = {\n",
    "            'model': lasso_grid_search,\n",
    "            'train_r2': train_r2,\n",
    "            'test_r2': test_r2,\n",
    "            'test_rmse': test_rmse,\n",
    "            'test_mae': test_mae,\n",
    "            'best_params': lasso_grid_search.best_params_,\n",
    "            'best_score': lasso_grid_search.best_score_\n",
    "        }\n",
    "        \n",
    "        self.models['lasso'] = results\n",
    "        \n",
    "        logger.info(f\"Lasso Model - Train R²: {train_r2:.4f}, Test R²: {test_r2:.4f}\")\n",
    "        logger.info(f\"Lasso Model - Test RMSE: {test_rmse:.4f}, Test MAE: {test_mae:.4f}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def train_random_forest_model(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Train Random Forest model with hyperparameter tuning.\n",
    "        \n",
    "        Returns:\n",
    "            Dict[str, Any]: Model results and metrics\n",
    "        \"\"\"\n",
    "        logger.info(\"Training Random Forest model...\")\n",
    "        \n",
    "        # Define parameter grid\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 80, 100, 150],\n",
    "            'max_depth': [3, 5, 7, 10],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "        \n",
    "        # Grid search\n",
    "        rf = RandomForestRegressor(random_state=42)\n",
    "        rf_grid_search = GridSearchCV(\n",
    "            rf, param_grid, scoring='r2', cv=3, n_jobs=-1\n",
    "        )\n",
    "        rf_grid_search.fit(self.X_train_scaled, self.y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_train_pred = rf_grid_search.predict(self.X_train_scaled)\n",
    "        y_test_pred = rf_grid_search.predict(self.X_test_scaled)\n",
    "        \n",
    "        # Metrics\n",
    "        train_r2 = r2_score(self.y_train, y_train_pred)\n",
    "        test_r2 = r2_score(self.y_test, y_test_pred)\n",
    "        test_rmse = np.sqrt(mean_squared_error(self.y_test, y_test_pred))\n",
    "        test_mae = mean_absolute_error(self.y_test, y_test_pred)\n",
    "        \n",
    "        results = {\n",
    "            'model': rf_grid_search,\n",
    "            'train_r2': train_r2,\n",
    "            'test_r2': test_r2,\n",
    "            'test_rmse': test_rmse,\n",
    "            'test_mae': test_mae,\n",
    "            'best_params': rf_grid_search.best_params_,\n",
    "            'best_score': rf_grid_search.best_score_,\n",
    "            'feature_importance': rf_grid_search.best_estimator_.feature_importances_\n",
    "        }\n",
    "        \n",
    "        self.models['random_forest'] = results\n",
    "        \n",
    "        logger.info(f\"Random Forest - Train R²: {train_r2:.4f}, Test R²: {test_r2:.4f}\")\n",
    "        logger.info(f\"Random Forest - Test RMSE: {test_rmse:.4f}, Test MAE: {test_mae:.4f}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def train_xgboost_model(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Train XGBoost model with hyperparameter tuning.\n",
    "        \n",
    "        Returns:\n",
    "            Dict[str, Any]: Model results and metrics\n",
    "        \"\"\"\n",
    "        logger.info(\"Training XGBoost model...\")\n",
    "        \n",
    "        # Define parameter grid\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'subsample': [0.8, 0.9, 1.0],\n",
    "            'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "        }\n",
    "        \n",
    "        # Grid search\n",
    "        xgb = XGBRegressor(random_state=42, n_jobs=-1)\n",
    "        xgb_grid_search = GridSearchCV(\n",
    "            xgb, param_grid, scoring='r2', cv=3, n_jobs=-1\n",
    "        )\n",
    "        xgb_grid_search.fit(self.X_train_scaled, self.y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_train_pred = xgb_grid_search.predict(self.X_train_scaled)\n",
    "        y_test_pred = xgb_grid_search.predict(self.X_test_scaled)\n",
    "        \n",
    "        # Metrics\n",
    "        train_r2 = r2_score(self.y_train, y_train_pred)\n",
    "        test_r2 = r2_score(self.y_test, y_test_pred)\n",
    "        test_rmse = np.sqrt(mean_squared_error(self.y_test, y_test_pred))\n",
    "        test_mae = mean_absolute_error(self.y_test, y_test_pred)\n",
    "        \n",
    "        results = {\n",
    "            'model': xgb_grid_search,\n",
    "            'train_r2': train_r2,\n",
    "            'test_r2': test_r2,\n",
    "            'test_rmse': test_rmse,\n",
    "            'test_mae': test_mae,\n",
    "            'best_params': xgb_grid_search.best_params_,\n",
    "            'best_score': xgb_grid_search.best_score_,\n",
    "            'feature_importance': xgb_grid_search.best_estimator_.feature_importances_\n",
    "        }\n",
    "        \n",
    "        self.models['xgboost'] = results\n",
    "        \n",
    "        logger.info(f\"XGBoost - Train R²: {train_r2:.4f}, Test R²: {test_r2:.4f}\")\n",
    "        logger.info(f\"XGBoost - Test RMSE: {test_rmse:.4f}, Test MAE: {test_mae:.4f}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def plot_feature_importance(self, model_name: str = 'random_forest', \n",
    "                              save_path: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Plot feature importance for tree-based models.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Name of the model to plot importance for\n",
    "            save_path (str): Path to save the plot\n",
    "        \"\"\"\n",
    "        if model_name not in self.models:\n",
    "            logger.error(f\"Model {model_name} not found\")\n",
    "            return\n",
    "        \n",
    "        if 'feature_importance' not in self.models[model_name]:\n",
    "            logger.error(f\"Feature importance not available for {model_name}\")\n",
    "            return\n",
    "        \n",
    "        importances = self.models[model_name]['feature_importance']\n",
    "        indices = np.argsort(importances)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.title(f'Feature Importance - {model_name.replace(\"_\", \" \").title()}')\n",
    "        plt.barh(range(len(indices)), importances[indices], color='red', align='center')\n",
    "        plt.yticks(range(len(indices)), [self.feature_names[i] for i in indices])\n",
    "        plt.xlabel('Relative Importance')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def compare_models(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Compare performance of all trained models.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Comparison table\n",
    "        \"\"\"\n",
    "        comparison_data = []\n",
    "        \n",
    "        for model_name, results in self.models.items():\n",
    "            comparison_data.append({\n",
    "                'Model': model_name.replace('_', ' ').title(),\n",
    "                'Train R²': f\"{results['train_r2']:.4f}\",\n",
    "                'Test R²': f\"{results['test_r2']:.4f}\",\n",
    "                'Test RMSE': f\"{results['test_rmse']:.4f}\",\n",
    "                'Test MAE': f\"{results['test_mae']:.4f}\",\n",
    "                'Best CV Score': f\"{results['best_score']:.4f}\"\n",
    "            })\n",
    "        \n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        logger.info(\"Model Comparison:\")\n",
    "        logger.info(f\"\\n{comparison_df.to_string(index=False)}\")\n",
    "        \n",
    "        return comparison_df\n",
    "    \n",
    "    def save_best_model(self, model_name: str = 'xgboost', filepath: str = 'best_model.pkl'):\n",
    "        \"\"\"\n",
    "        Save the best performing model.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Name of the model to save\n",
    "            filepath (str): Path to save the model\n",
    "        \"\"\"\n",
    "        if model_name not in self.models:\n",
    "            logger.error(f\"Model {model_name} not found\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            model = self.models[model_name]['model'].best_estimator_\n",
    "            with open(filepath, 'wb') as f:\n",
    "                pickle.dump({\n",
    "                    'model': model,\n",
    "                    'scaler': self.scaler,\n",
    "                    'imputer': self.imputer,\n",
    "                    'feature_names': self.feature_names,\n",
    "                    'model_name': model_name,\n",
    "                    'performance': self.models[model_name]\n",
    "                }, f)\n",
    "            \n",
    "            logger.info(f\"Best model saved to {filepath}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving model: {str(e)}\")\n",
    "    \n",
    "    def predict(self, new_data: pd.DataFrame, model_name: str = 'xgboost') -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Make predictions on new data.\n",
    "        \n",
    "        Args:\n",
    "            new_data (pd.DataFrame): New data for prediction\n",
    "            model_name (str): Name of the model to use\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Predictions\n",
    "        \"\"\"\n",
    "        if model_name not in self.models:\n",
    "            raise ValueError(f\"Model {model_name} not found\")\n",
    "        \n",
    "        # Preprocess new data\n",
    "        new_data_scaled = self.scaler.transform(new_data)\n",
    "        new_data_scaled = self.imputer.transform(new_data_scaled)\n",
    "        \n",
    "        # Make predictions\n",
    "        model = self.models[model_name]['model'].best_estimator_\n",
    "        predictions = model.predict(new_data_scaled)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def create_visualizations(self, save_dir: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Create comprehensive visualizations for the analysis.\n",
    "        \n",
    "        Args:\n",
    "            save_dir (str): Directory to save plots\n",
    "        \"\"\"\n",
    "        logger.info(\"Creating visualizations...\")\n",
    "        \n",
    "        # 1. Price trend over time\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        self.dataset.set_index('Date')['EUR/USD'].plot()\n",
    "        plt.title('Gold Price Trend Over Time')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Price (EUR/USD)')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_dir:\n",
    "            plt.savefig(f\"{save_dir}/price_trend.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # 2. Distribution plots\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        numeric_cols = self.dataset.drop('Date', axis=1).columns\n",
    "        \n",
    "        for i, col in enumerate(numeric_cols):\n",
    "            row, col_idx = i // 3, i % 3\n",
    "            sns.histplot(data=self.dataset, x=col, kde=True, ax=axes[row, col_idx])\n",
    "            axes[row, col_idx].set_title(f'Distribution of {col}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        if save_dir:\n",
    "            plt.savefig(f\"{save_dir}/distributions.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # 3. Box plots\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        \n",
    "        for i, col in enumerate(numeric_cols):\n",
    "            row, col_idx = i // 3, i % 3\n",
    "            sns.boxplot(data=self.dataset, x=col, ax=axes[row, col_idx], color='violet')\n",
    "            axes[row, col_idx].set_title(f'Box Plot of {col}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        if save_dir:\n",
    "            plt.savefig(f\"{save_dir}/boxplots.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # 4. Model comparison\n",
    "        if self.models:\n",
    "            model_names = list(self.models.keys())\n",
    "            test_r2_scores = [self.models[name]['test_r2'] for name in model_names]\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            bars = plt.bar(model_names, test_r2_scores, color=['red', 'blue', 'green'])\n",
    "            plt.title('Model Performance Comparison (Test R²)')\n",
    "            plt.ylabel('R² Score')\n",
    "            plt.ylim(0, 1)\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for bar, score in zip(bars, test_r2_scores):\n",
    "                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                        f'{score:.4f}', ha='center', va='bottom')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            if save_dir:\n",
    "                plt.savefig(f\"{save_dir}/model_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the complete gold price prediction pipeline.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize predictor\n",
    "        predictor = GoldPricePredictor(\"gold_price_data.csv\")\n",
    "        \n",
    "        # Load and explore data\n",
    "        predictor.load_data()\n",
    "        predictor.explore_data()\n",
    "        \n",
    "        # Create initial visualizations\n",
    "        predictor.create_correlation_heatmap()\n",
    "        \n",
    "        # Preprocess data\n",
    "        predictor.dataset = predictor.preprocess_data()\n",
    "        \n",
    "        # Prepare features\n",
    "        X, y = predictor.prepare_features()\n",
    "        \n",
    "        # Split and scale data\n",
    "        predictor.split_and_scale_data(X, y)\n",
    "        \n",
    "        # Train models\n",
    "        predictor.train_lasso_model()\n",
    "        predictor.train_random_forest_model()\n",
    "        predictor.train_xgboost_model()\n",
    "        \n",
    "        # Compare models\n",
    "        comparison = predictor.compare_models()\n",
    "        \n",
    "        # Create visualizations\n",
    "        predictor.create_visualizations()\n",
    "        \n",
    "        # Plot feature importance for best model\n",
    "        predictor.plot_feature_importance('random_forest')\n",
    "        predictor.plot_feature_importance('xgboost')\n",
    "        \n",
    "        # Save best model\n",
    "        predictor.save_best_model('xgboost', 'gold_price_model.pkl')\n",
    "        \n",
    "        logger.info(\"Gold price prediction pipeline completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main pipeline: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605e6388-d471-4f34-9f08-205c6aa27895",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
